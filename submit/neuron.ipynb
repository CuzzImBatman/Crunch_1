{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEm3ItwiZW5q"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/broad-1/quickstarters/random-submission/random-submission.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uKUtqkoZW5u"
   },
   "source": [
    "![Cover](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/broad-1/assets/cover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xapBDgCs_mX"
   },
   "source": [
    "Change in load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:58:29.091837Z",
     "start_time": "2024-10-28T19:58:17.956941Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1R6OFCNZW5v",
    "outputId": "431f755c-08e6-4d32-a827-7e0f6486383a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crunch-cli\n",
      "  Downloading crunch_cli-5.7.3-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (8.1.7)\n",
      "Collecting coloredlogs (from crunch-cli)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dataclasses_json (from crunch-cli)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (8.5.0)\n",
      "Collecting inquirer (from crunch-cli)\n",
      "  Downloading inquirer-3.4.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.4.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (3.4.2)\n",
      "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (24.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (2.2.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (5.9.5)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (17.0.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (8.3.4)\n",
      "Collecting python-dotenv (from crunch-cli)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (6.0.2)\n",
      "Collecting redbaron (from crunch-cli)\n",
      "  Downloading redbaron-0.9.2-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: requirements-parser in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (0.9.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (4.66.6)\n",
      "Collecting spatialdata-plot (from crunch-cli)\n",
      "  Downloading spatialdata_plot-0.2.8-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->crunch-cli)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json->crunch-cli)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json->crunch-cli)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata->crunch-cli) (3.21.0)\n",
      "Collecting blessed>=1.19.0 (from inquirer->crunch-cli)\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting editor>=1.6.0 (from inquirer->crunch-cli)\n",
      "  Downloading editor-1.6.6-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting readchar>=4.2.0 (from inquirer->crunch-cli)\n",
      "  Downloading readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (2024.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (2.2.1)\n",
      "Collecting baron>=0.7 (from redbaron->crunch-cli)\n",
      "  Downloading baron-0.10.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (2024.8.30)\n",
      "Requirement already satisfied: types-setuptools>=69.1.0 in /usr/local/lib/python3.10/dist-packages (from requirements-parser->crunch-cli) (75.6.0.20241126)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->crunch-cli) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from spatialdata-plot->crunch-cli) (3.8.0)\n",
      "Collecting matplotlib-scalebar (from spatialdata-plot->crunch-cli)\n",
      "  Downloading matplotlib_scalebar-0.8.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting scanpy (from spatialdata-plot->crunch-cli)\n",
      "  Downloading scanpy-1.10.4-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting spatialdata>=0.2.6 (from spatialdata-plot->crunch-cli)\n",
      "  Downloading spatialdata-0.2.6-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting rply (from baron>=0.7->redbaron->crunch-cli)\n",
      "  Downloading rply-0.7.8-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer->crunch-cli) (0.2.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer->crunch-cli) (1.16.0)\n",
      "Collecting runs (from editor>=1.6.0->inquirer->crunch-cli)\n",
      "  Downloading runs-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xmod (from editor>=1.6.0->inquirer->crunch-cli)\n",
      "  Downloading xmod-1.8.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting anndata>=0.9.1 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading anndata-0.11.1-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting dask-image (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading dask_image-2024.5.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: dask>=2024.4.1 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.10.0)\n",
      "Collecting fsspec<=2023.6 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: geopandas>=0.14 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.1)\n",
      "Collecting multiscale-spatial-image>=2.0.2 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading multiscale_spatial_image-2.0.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.60.0)\n",
      "Collecting ome-zarr>=0.8.4 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading ome_zarr-0.9.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.8.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (13.9.4)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.24.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (75.1.0)\n",
      "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.0.6)\n",
      "Collecting spatial-image>=1.1.0 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading spatial_image-1.1.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.12.2)\n",
      "Collecting xarray-schema (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading xarray_schema-0.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting xarray-spatial>=0.3.5 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading xarray_spatial-0.4.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: xarray>=2024.10.0 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.10.0)\n",
      "Collecting zarr<3 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading zarr-2.18.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json->crunch-cli)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (3.2.0)\n",
      "Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (3.12.1)\n",
      "Collecting legacy-api-wrap>=1.4 (from scanpy->spatialdata-plot->crunch-cli)\n",
      "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (8.4.0)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (1.0.1)\n",
      "Collecting pynndescent>=0.5 (from scanpy->spatialdata-plot->crunch-cli)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (0.13.2)\n",
      "Collecting session-info (from scanpy->spatialdata-plot->crunch-cli)\n",
      "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (0.14.4)\n",
      "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy->spatialdata-plot->crunch-cli)\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.9.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading array_api_compat-1.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.4.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.12.1)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.7.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.43.0)\n",
      "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.11.9)\n",
      "Collecting distributed (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading distributed-2024.12.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting xarray-dataclasses>=1.1.0 (from spatial-image>=1.1.0->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading xarray_dataclasses-1.9.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting datashader>=0.15.0 (from xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading datashader-0.16.3-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting asciitree (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting numcodecs>=0.10.0 (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting fasteners (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pims>=0.4.1 (from dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading pims-0.7.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tifffile>=2018.10.18 in /usr/local/lib/python3.10/dist-packages (from dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.9.20)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.3.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.18.0)\n",
      "Collecting appdirs (from rply->baron>=0.7->redbaron->crunch-cli)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.36.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.4)\n",
      "Collecting stdlib_list (from session-info->scanpy->spatialdata-plot->crunch-cli)\n",
      "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.18.3)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[array,dataframe]>=2024.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.0)\n",
      "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.1.1)\n",
      "Collecting pyct (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading pyct-0.5.0-py2.py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading s3fs-2024.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.1.2)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.0)\n",
      "Collecting slicerator>=0.9.8 (from pims>=0.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading slicerator-1.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "INFO: pip is looking at multiple versions of distributed to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting distributed (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading distributed-2024.11.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading distributed-2024.11.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading distributed-2024.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading distributed-2024.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.4)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.1.0)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (6.3.3)\n",
      "Collecting zict>=3.0.0 (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[array,dataframe]>=2024.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.2)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading aiobotocore-2.15.2-py3-none-any.whl.metadata (23 kB)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading s3fs-2024.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.6.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: pip is still looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.7.0 (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.5.4 (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.17.0)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->crunch-cli)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading crunch_cli-5.7.3-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading inquirer-3.4.0-py3-none-any.whl (18 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading redbaron-0.9.2-py2.py3-none-any.whl (34 kB)\n",
      "Downloading spatialdata_plot-0.2.8-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading baron-0.10.1-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading editor-1.6.6-py3-none-any.whl (4.0 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading readchar-4.2.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading spatialdata-0.2.6-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.7/168.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading matplotlib_scalebar-0.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading scanpy-1.10.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anndata-0.11.1-py3-none-any.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
      "Downloading multiscale_spatial_image-2.0.2-py3-none-any.whl (29 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading ome_zarr-0.9.0-py3-none-any.whl (37 kB)\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spatial_image-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xarray_spatial-0.4.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zarr-2.18.3-py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dask_image-2024.5.3-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rply-0.7.8-py2.py3-none-any.whl (16 kB)\n",
      "Downloading runs-1.2.2-py3-none-any.whl (7.0 kB)\n",
      "Downloading xarray_schema-0.0.3-py3-none-any.whl (10 kB)\n",
      "Downloading xmod-1.8.1-py3-none-any.whl (4.6 kB)\n",
      "Downloading array_api_compat-1.9.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datashader-0.16.3-py2.py3-none-any.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xarray_dataclasses-1.9.1-py3-none-any.whl (14 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading distributed-2024.10.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicerator-1.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
      "Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: session-info, pims, asciitree\n",
      "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=d34b408b24cd2a5b9b42bb7811818ae8c4b6986afd1fde4141a745f4496e31fe\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
      "  Building wheel for pims (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pims: filename=PIMS-0.7-py3-none-any.whl size=84591 sha256=4025fddea73c830e534c24e558338c1a3d9d35e0516c47b9a57430fcf53e8bef\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/cb/f1/939f4adc0c5bcb1a1a78566d67869368d3d8dc8abd84f63c38\n",
      "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5034 sha256=80774c943eea426283030afedb659004228f339e9097ef81ab489c7845cc388b\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
      "Successfully built session-info pims asciitree\n",
      "Installing collected packages: sortedcontainers, slicerator, asciitree, appdirs, zict, xmod, urllib3, tblib, stdlib_list, rply, readchar, python-dotenv, pyct, numcodecs, mypy-extensions, marshmallow, legacy-api-wrap, jmespath, humanfriendly, fsspec, fasteners, blessed, array-api-compat, aioitertools, zarr, typing-inspect, session-info, runs, pims, coloredlogs, botocore, baron, redbaron, pynndescent, matplotlib-scalebar, editor, distributed, dataclasses_json, dask-expr, anndata, xarray-schema, xarray-dataclasses, umap-learn, inquirer, datashader, aiobotocore, xarray-spatial, spatial-image, scanpy, s3fs, dask-image, multiscale-spatial-image, ome-zarr, spatialdata, spatialdata-plot, crunch-cli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.5.4 aioitertools-0.12.0 anndata-0.11.1 appdirs-1.4.4 array-api-compat-1.9.1 asciitree-0.3.3 baron-0.10.1 blessed-1.20.0 botocore-1.31.17 coloredlogs-15.0.1 crunch-cli-5.7.3 dask-expr-1.1.16 dask-image-2024.5.3 dataclasses_json-0.6.7 datashader-0.16.3 distributed-2024.10.0 editor-1.6.6 fasteners-0.19 fsspec-2023.6.0 humanfriendly-10.0 inquirer-3.4.0 jmespath-1.0.1 legacy-api-wrap-1.4.1 marshmallow-3.23.1 matplotlib-scalebar-0.8.1 multiscale-spatial-image-2.0.2 mypy-extensions-1.0.0 numcodecs-0.13.1 ome-zarr-0.9.0 pims-0.7 pyct-0.5.0 pynndescent-0.5.13 python-dotenv-1.0.1 readchar-4.2.1 redbaron-0.9.2 rply-0.7.8 runs-1.2.2 s3fs-2023.6.0 scanpy-1.10.4 session-info-1.0.0 slicerator-1.1.0 sortedcontainers-2.4.0 spatial-image-1.1.0 spatialdata-0.2.6 spatialdata-plot-0.2.8 stdlib_list-0.11.0 tblib-3.0.0 typing-inspect-0.9.0 umap-learn-0.5.7 urllib3-1.26.20 xarray-dataclasses-1.9.1 xarray-schema-0.0.3 xarray-spatial-0.4.0 xmod-1.8.1 zarr-2.18.3 zict-3.0.0\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade crunch-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM-OUR9uZW5w"
   },
   "source": [
    "Get a new token: https://hub.crunchdao.com/competitions/broad-1/submit/via/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5E-1uPtmGPZs",
    "outputId": "9da3da78-e032-4394-823d-18546ec11059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: crunch setup [OPTIONS] COMPETITION_NAME PROJECT_NAME [DIRECTORY]\n",
      "\n",
      "  Setup a workspace directory with the latest submission of you code.\n",
      "\n",
      "Options:\n",
      "  --token TEXT                   Clone token to use.  [required]\n",
      "  --submission INTEGER           Submission number to clone. (latest if not\n",
      "                                 specified)\n",
      "  --no-data                      Do not download the data. (faster)\n",
      "  --no-model                     Do not download the model of the cloned\n",
      "                                 submission.\n",
      "  -f, --force                    Deleting the old directory (if any).\n",
      "  --model-directory TEXT         Directory where your model is stored.\n",
      "                                 [default: resources]\n",
      "  --no-quickstarter              Disable quickstarter selection.\n",
      "  --quickstarter-name TEXT       Pre-select a quickstarter.\n",
      "  --show-notebook-quickstarters  Show quickstarters notebook in selection.\n",
      "  --notebook                     Setup everything for a notebook environment.\n",
      "  --size [default|large]         Use another data variant.\n",
      "  --help                         Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!crunch setup --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:09:16.219026Z",
     "start_time": "2024-10-28T20:01:12.655472Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQF_P04CZW5w",
    "outputId": "bc8963e3-2f3a-40e9-ff6a-087ddc03d0b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/15254/main.py (57629 bytes)\n",
      "notebook.ipynb: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/15254/notebook.ipynb (77431 bytes)\n",
      "requirements.txt: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/15254/requirements.original.txt (302 bytes)\n",
      "resources/checkpoint_epoch_219.pth.tar: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/models/16455/checkpoint_epoch_219.pth.tar (255794751 bytes)\n",
      "resources/pytorch_model.bin: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/models/16455/pytorch_model.bin (1213527781 bytes)\n",
      "data/DC1.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/DC1.zarr.zip (720810785 bytes)\n",
      "data/DC1.zarr.zip: uncompress into data/DC1.zarr.zip.7l51mdy1\n",
      "data/UC1_NI.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC1_NI.zarr.zip (889729934 bytes)\n",
      "data/UC1_NI.zarr.zip: uncompress into data/UC1_NI.zarr.zip.0r1qley8\n",
      "data/UC6_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC6_I.zarr.zip (923581852 bytes)\n",
      "data/UC6_I.zarr.zip: uncompress into data/UC6_I.zarr.zip.e3bosvwz\n",
      "data/UC7_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC7_I.zarr.zip (769304631 bytes)\n",
      "data/UC7_I.zarr.zip: uncompress into data/UC7_I.zarr.zip.twfjo1y7\n",
      "data/UC6_NI.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC6_NI.zarr.zip (706148035 bytes)\n",
      "data/UC6_NI.zarr.zip: uncompress into data/UC6_NI.zarr.zip.eiumjbah\n",
      "data/UC1_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC1_I.zarr.zip (926772130 bytes)\n",
      "data/UC1_I.zarr.zip: uncompress into data/UC1_I.zarr.zip.3emjwb32\n",
      "data/UC9_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC9_I.zarr.zip (927207232 bytes)\n",
      "data/UC9_I.zarr.zip: uncompress into data/UC9_I.zarr.zip._i0bqti1\n",
      "data/DC5.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/DC5.zarr.zip (813303073 bytes)\n",
      "data/DC5.zarr.zip: uncompress into data/DC5.zarr.zip.qoo97v40\n",
      "\n",
      "---\n",
      "Success! Your environment has been correctly setup.\n",
      "Next recommended actions:\n",
      " - To see all of the available commands of the CrunchDAO CLI, run: crunch --help\n"
     ]
    }
   ],
   "source": [
    "# !crunch setup --notebook --size default broad-1 test --token tPM1wCmTDAChUfvdjixZEGdh9BvAH39jHXq1aG8Xg8UVMyvs64TjPQLjV90hkVDq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66KdKhioGPZs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2wH4d5jGPZt"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:13:12.168568Z",
     "start_time": "2024-10-28T20:13:11.467039Z"
    },
    "id": "ix40zGjWZW5y"
   },
   "outputs": [],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgotR9FeGPZt"
   },
   "source": [
    "### Save pre_load of cells for each image ( position, id and GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:10:16.861852Z",
     "start_time": "2024-10-28T20:10:10.463380Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vPUsacWZW5x",
    "outputId": "ac4bbf2a-83f0-4e4d-d5ea-422414a7d824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spatialdata in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
      "Requirement already satisfied: anndata>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (0.11.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from spatialdata) (8.1.7)\n",
      "Requirement already satisfied: dask-image in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2024.5.3)\n",
      "Requirement already satisfied: dask>=2024.4.1 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2024.10.0)\n",
      "Requirement already satisfied: fsspec<=2023.6 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2023.6.0)\n",
      "Requirement already satisfied: geopandas>=0.14 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (1.0.1)\n",
      "Requirement already satisfied: multiscale-spatial-image>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2.0.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from spatialdata) (3.4.2)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from spatialdata) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spatialdata) (1.26.4)\n",
      "Requirement already satisfied: ome-zarr>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (0.9.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2.2.2)\n",
      "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from spatialdata) (1.8.2)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from spatialdata) (17.0.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from spatialdata) (13.9.4)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from spatialdata) (0.24.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spatialdata) (1.13.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spatialdata) (75.1.0)\n",
      "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2.0.6)\n",
      "Requirement already satisfied: spatial-image>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (4.12.2)\n",
      "Requirement already satisfied: xarray-schema in /usr/local/lib/python3.10/dist-packages (from spatialdata) (0.0.3)\n",
      "Requirement already satisfied: xarray-spatial>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (0.4.0)\n",
      "Requirement already satisfied: xarray>=2024.10.0 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2024.10.0)\n",
      "Requirement already satisfied: zarr<3 in /usr/local/lib/python3.10/dist-packages (from spatialdata) (2.18.3)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.9.1->spatialdata) (1.9.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.9.1->spatialdata) (1.2.2)\n",
      "Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.9.1->spatialdata) (3.12.1)\n",
      "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata>=0.9.1->spatialdata) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.9.1->spatialdata) (24.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata) (3.1.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata) (8.5.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14->spatialdata) (0.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14->spatialdata) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from multiscale-spatial-image>=2.0.2->spatialdata) (2.8.2)\n",
      "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from ome-zarr>=0.8.4->spatialdata) (3.11.9)\n",
      "Requirement already satisfied: distributed in /usr/local/lib/python3.10/dist-packages (from ome-zarr>=0.8.4->spatialdata) (2024.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ome-zarr>=0.8.4->spatialdata) (2.32.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spatialdata) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->spatialdata) (2024.2)\n",
      "Requirement already satisfied: xarray-dataclasses>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from spatial-image>=1.1.0->spatialdata) (1.9.1)\n",
      "Requirement already satisfied: datashader>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from xarray-spatial>=0.3.5->spatialdata) (0.16.3)\n",
      "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr<3->spatialdata) (0.3.3)\n",
      "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr<3->spatialdata) (0.13.1)\n",
      "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr<3->spatialdata) (0.19)\n",
      "Requirement already satisfied: pims>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from dask-image->spatialdata) (0.7)\n",
      "Requirement already satisfied: tifffile>=2018.10.18 in /usr/local/lib/python3.10/dist-packages (from dask-image->spatialdata) (2024.9.20)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->spatialdata) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->spatialdata) (4.3.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->spatialdata) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->spatialdata) (2.18.0)\n",
      "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->spatialdata) (11.0.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->spatialdata) (2.36.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->spatialdata) (0.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata) (1.18.3)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2024.4.1->dask-image->spatialdata) (1.1.16)\n",
      "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (3.1.0)\n",
      "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (1.0.0)\n",
      "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (2.1.1)\n",
      "Requirement already satisfied: pyct in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (0.5.0)\n",
      "Requirement already satisfied: s3fs in /usr/local/lib/python3.10/dist-packages (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (2023.6.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2024.4.1->spatialdata) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->spatialdata) (0.1.2)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2024.4.1->spatialdata) (1.0.0)\n",
      "Requirement already satisfied: slicerator>=0.9.8 in /usr/local/lib/python3.10/dist-packages (from pims>=0.4.1->dask-image->spatialdata) (1.1.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyogrio>=0.7.2->geopandas>=0.14->spatialdata) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->multiscale-spatial-image>=2.0.2->spatialdata) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ome-zarr>=0.8.4->spatialdata) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ome-zarr>=0.8.4->spatialdata) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ome-zarr>=0.8.4->spatialdata) (1.26.20)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata) (3.1.4)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata) (5.9.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata) (6.3.3)\n",
      "Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed->ome-zarr>=0.8.4->spatialdata) (3.0.2)\n",
      "Requirement already satisfied: aiobotocore~=2.5.0 in /usr/local/lib/python3.10/dist-packages (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (2.5.4)\n",
      "Requirement already satisfied: botocore<1.31.18,>=1.31.17 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (1.31.17)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (1.17.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (0.12.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install spatialdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:12:18.447106Z",
     "start_time": "2024-10-28T20:11:44.024001Z"
    },
    "id": "dxpjie7mZW5x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spatialdata as sd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from skimage.measure import regionprops\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H6lw0abGPZu"
   },
   "outputs": [],
   "source": [
    "# sdata_dir=f'F:/DATA/crunch_large/zip_server\n",
    "\n",
    "# pre_load_path= './tmp/pre_load'\n",
    "\n",
    "# os.makedirs(pre_load_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldVyLnp6GPZv"
   },
   "outputs": [],
   "source": [
    "# dir = f'D:/data/crunch_large/data/'\n",
    "def create_pre_load(pre_load_path,sdata_dir= '/data',name_list=['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I']):\n",
    "    NAMES = name_list\n",
    "    # names = sample_names[2:]\n",
    "    for name in NAMES:\n",
    "        pkl_file_path= f'{pre_load_path}/{name}_cells.pkl'\n",
    "        if os.path.exists(pkl_file_path):\n",
    "            continue\n",
    "        with open(pkl_file_path,'wb') as f:\n",
    "            print(f\"{sdata_dir}/{name}.zarr\")\n",
    "            sdata = sd.read_zarr(f\"{sdata_dir}/{name}.zarr\")\n",
    "            cell_id_group=sdata['cell_id-group']\n",
    "\n",
    "        # anucleus= sdata['anucleus']\n",
    "\n",
    "            cell_id_train =  cell_id_group.obs[ cell_id_group.obs['group'] == 'train']['cell_id'].to_numpy()\n",
    "            # cell_id_train = list(set(cell_id_train).intersection(set( anucleus.obs['cell_id'].unique())))\n",
    "\n",
    "            cell_id_test= cell_id_group.obs[ cell_id_group.obs['group'] == 'test']['cell_id']\n",
    "            cell_id_validation= cell_id_group.obs[ cell_id_group.obs['group'] == 'validation']['cell_id']\n",
    "            max_len= max(max(cell_id_train),max(cell_id_test),max(cell_id_validation))\n",
    "            bool_list= [0]*(max_len+1)\n",
    "\n",
    "            for id in cell_id_train:\n",
    "                bool_list[id]=1\n",
    "            for id in cell_id_test:\n",
    "                bool_list[id]=2\n",
    "            for id in cell_id_validation:\n",
    "                bool_list[id]=3\n",
    "            ground_truth=np.array([])\n",
    "            if name != 'DC1':\n",
    "                anucleus= sdata['anucleus']\n",
    "                cell_id_train = list(set(cell_id_train).intersection(set( anucleus.obs['cell_id'].unique())))\n",
    "                ground_truth =  anucleus.layers['counts'][ anucleus.obs['cell_id'].isin(cell_id_train),:]\n",
    "            r=10\n",
    "            # im= sdata['HE_registered'].to_numpy()\n",
    "            patches_list = []  # Initialize an empty list to store patches\n",
    "            index=0\n",
    "\n",
    "            cell_list=[]\n",
    "            for props in tqdm( regionprops(sdata['HE_nuc_original'][0, :, :].to_numpy()) ):\n",
    "                cell_item={}\n",
    "                cell_item['cell_id']= props.label\n",
    "                centroid = props.centroid\n",
    "                cell_item['center']=[int(centroid[0]), int(centroid[1])]\n",
    "                if cell_item['cell_id'] >= len(bool_list):\n",
    "                    cell_item['label']='None'\n",
    "                    cell_item['anucleus']=np.array([])\n",
    "                    continue\n",
    "                elif bool_list[props.label] ==1:\n",
    "                    if len(ground_truth)!=0:\n",
    "                        cell_item['anucleus']=ground_truth[index]\n",
    "                        index+=1\n",
    "                    else:\n",
    "                        cell_item['anucleus']=np.array([])\n",
    "                    cell_item['label']='train'\n",
    "                elif bool_list[props.label] ==2:\n",
    "                    cell_item['anucleus']=np.array([])\n",
    "                    cell_item['label']='test'\n",
    "                elif bool_list[props.label] ==3:\n",
    "                    cell_item['anucleus']=np.array([])\n",
    "                    cell_item['label']='validation'\n",
    "                else:\n",
    "                    cell_item['anucleus']=np.array([])\n",
    "                    cell_item['label']='None'\n",
    "                cell_list.append(cell_item)\n",
    "            sdata=None\n",
    "            pickle.dump(cell_list,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAdb6yQzGPZw"
   },
   "source": [
    "### Create cluster cells for each H&E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODZ49KjgGPZw"
   },
   "outputs": [],
   "source": [
    "# cluster_path= './tmp/cluster'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fK4DCbVpGPZw"
   },
   "outputs": [],
   "source": [
    "def create_cluster(group,n_centroid,NAMES,pre_load_path,cluster_path):\n",
    "    for name in NAMES:\n",
    "        with open(f'{pre_load_path}/{name}_cells.pkl','rb') as f:\n",
    "            cell_list= pickle.load(f)\n",
    "\n",
    "        if group =='evel':\n",
    "            filtered_cells = [c for c in cell_list if c['label'] in ('validation', 'test')]\n",
    "        else:\n",
    "            filtered_cells = [c for c in cell_list if c['label'] == group]\n",
    "\n",
    "        cell_locations = pd.DataFrame([{'x': c['center'][1], 'y': c['center'][0],'cell_id': c['cell_id'],'counts':c['anucleus']} for c in filtered_cells])\n",
    "\n",
    "        # Define the number of clusters\n",
    "        n_clusters = n_centroid  # Adjust this based on your data and requirements\n",
    "\n",
    "        # Perform K-Means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(cell_locations[['x', 'y']])\n",
    "\n",
    "        # Add the cluster labels to the DataFrame\n",
    "        cell_locations['cluster'] = kmeans.labels_\n",
    "\n",
    "        # Visualize the clusters\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(\n",
    "            cell_locations['x'],\n",
    "            cell_locations['y'],\n",
    "            c=cell_locations['cluster'],\n",
    "            s=0.5,\n",
    "            cmap='viridis',  # Continuous colormap\n",
    "            alpha=0.5\n",
    "        )\n",
    "        plt.colorbar(label='Cluster ID')  # Add colorbar to interpret color mapping\n",
    "        plt.title('Clusters Visualized with Continuous Colormap')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.show()\n",
    "        dir= f'{cluster_path}/{group}/cluster_data'\n",
    "        os.makedirs(f'{dir}',exist_ok= True )\n",
    "        with open(f'{dir}/{name}_cells.pkl','wb') as f:\n",
    "            pickle.dump(cell_locations,f)\n",
    "        with open(f'{dir}/{name}_kmeans.pkl','wb') as f:\n",
    "            pickle.dump(kmeans,f)\n",
    "        data = cell_locations[['x', 'y']].to_numpy()\n",
    "        clusters = cell_locations['cluster'].to_numpy()\n",
    "        centroids = kmeans.cluster_centers_\n",
    "\n",
    "        # Initialize a dictionary to store the radius of each cluster\n",
    "        cluster_radii = {}\n",
    "\n",
    "        # Calculate radius for each cluster\n",
    "        for cluster_id in range(len(centroids)):\n",
    "            # Get points belonging to the current cluster\n",
    "            cluster_points = data[clusters == cluster_id]\n",
    "\n",
    "            # Calculate distances to the cluster centroid\n",
    "            distances = np.linalg.norm(cluster_points - centroids[cluster_id], axis=1)\n",
    "\n",
    "            # Radius is the maximum distance\n",
    "            cluster_radii[cluster_id] = np.max(distances)\n",
    "\n",
    "        # Print radii for all clusters\n",
    "        list_r=[]\n",
    "        for cluster_id, radius in cluster_radii.items():\n",
    "            print(f\"Cluster {cluster_id}: Radius = {radius}\")\n",
    "            list_r.append(radius)\n",
    "        print(f'mean radius: {np.mean(list_r)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHafM7lTGPZx"
   },
   "source": [
    "#### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaaqSW8IGPZx",
    "outputId": "5dd3320b-36bd-46c2-9f3f-712f73e6776d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# create_cluster('train',n_centroid=6500,)\n",
    "print('a')\n",
    "def split_cluster(cluster_path,NAMES):\n",
    "      group='train'\n",
    "      for name in NAMES:\n",
    "            dir= f'{cluster_path}/{group}/cluster_data'\n",
    "            with open(f'{dir}/{name}_cells.pkl','rb') as f:\n",
    "                  cell_locations=pickle.load(f)\n",
    "            with open(f'{dir}/{name}_kmeans.pkl','rb') as f:\n",
    "                  kmeans=pickle.load(f)\n",
    "            data = cell_locations[['x', 'y']].to_numpy()\n",
    "            clusters = cell_locations['cluster'].to_numpy()\n",
    "            centroids = kmeans.cluster_centers_\n",
    "\n",
    "            cell_locations['train']=[1]*kmeans.labels_.shape[0]\n",
    "            # Initialize a dictionary to store the radius of each cluster\n",
    "            valid_cluster_id=[]\n",
    "            valid_centroid=[]\n",
    "            # Calculate radius for each cluster\n",
    "            for cluster_id in range(len(centroids)):\n",
    "                  x_center, y_center = centroids[cluster_id]\n",
    "\n",
    "                  half_side = int(32 / 2)\n",
    "                  x_min, x_max = x_center - half_side, x_center + half_side\n",
    "                  y_min, y_max = y_center - half_side, y_center + half_side\n",
    "\n",
    "                  index_cells_list_in_square =(\n",
    "                  (cell_locations['x'] >= x_min) & (cell_locations['x'] <= x_max) &\n",
    "                  (cell_locations['y'] >= y_min) & (cell_locations['y'] <= y_max)\n",
    "                  )\n",
    "                  # Get points belonging to the current cluster\n",
    "                  cluster_points = data[clusters == cluster_id]\n",
    "\n",
    "                  # Calculate distances to the cluster centroid\n",
    "                  if len(cluster_points)<=5 or len(index_cells_list_in_square)==0:\n",
    "                        cell_locations.loc[cell_locations['cluster'] == cluster_id, 'train']=-1\n",
    "                  # valid_cluster_id.append(cluster_id)\n",
    "                  # print(len(cluster_points))\n",
    "            unique_cluster_ids = cell_locations.loc[cell_locations['train'] == 1, 'cluster'].unique()\n",
    "\n",
    "            split = np.random.choice(unique_cluster_ids, int(0.1*len(unique_cluster_ids)), replace=False)\n",
    "            for cluster_id in split:\n",
    "                  cell_locations.loc[cell_locations['cluster'] == cluster_id, 'train']=0\n",
    "            split_dir=  f'{cluster_path}/{group}/cluster_data_split'\n",
    "            os.makedirs(split_dir,exist_ok= True )\n",
    "            with open(f'{split_dir}/{name}_cells.pkl', 'wb') as f:\n",
    "                  pickle.dump(cell_locations, f)\n",
    "            with open(f'{split_dir}/{name}_kmeans.pkl','wb') as f:\n",
    "                  pickle.dump(kmeans,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATn3mrAVGPZx"
   },
   "source": [
    "#### Validation & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDi1ZXVPGPZx"
   },
   "outputs": [],
   "source": [
    "# create_cluster('validation',n_centroid=600)\n",
    "# create_cluster('test',n_centroid=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv7dfbnVGPZy"
   },
   "source": [
    "## Crop patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uRPC4IyGPZy"
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GXVpX9VGPZy"
   },
   "outputs": [],
   "source": [
    "# crop_path='./tmp/patches'\n",
    "# r=80/2\n",
    "def crop_patches(sdata_dir, pre_load_path,crop_path,r,name_list=['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I'],evel=False):\n",
    "    NAMES = name_list\n",
    "    if evel == True:\n",
    "        group = 'evel'\n",
    "    else:\n",
    "        group= 'train'\n",
    "    for name in NAMES:\n",
    "\n",
    "        r=int(r)\n",
    "        save_dir= f'{crop_path}/{r*2}'\n",
    "        os.makedirs(f'{save_dir}/{group}',exist_ok=True)\n",
    "        if os.path.exists(f'{save_dir}/{group}/{name}.npy'):\n",
    "            continue\n",
    "        with open(f'{pre_load_path}/{name}_cells.pkl','rb') as f:\n",
    "                    cell_list= pickle.load(f)\n",
    "\n",
    "        sdata = sd.read_zarr(f\"{sdata_dir}/{name}.zarr\")\n",
    "        im= sdata['HE_original'].to_numpy()\n",
    "        patches_list = []  # Initialize an empty list to store patches\n",
    "\n",
    "\n",
    "        for props in cell_list:\n",
    "            if (evel== False and props['label'] == 'train') \\\n",
    "                or (evel == True and props['label'] in ('validation', 'test')):\n",
    "\n",
    "                cell_id= props['cell_id']\n",
    "                centroid = props['center']\n",
    "\n",
    "                x_center, y_center = centroid[1], centroid[0]\n",
    "                x_center= int(x_center)\n",
    "                y_center= int(y_center)\n",
    "                        # Calculate the crop boundaries\n",
    "\n",
    "\n",
    "                minr, maxr = y_center - r, y_center + r\n",
    "                minc, maxc = x_center - r, x_center + r\n",
    "\n",
    "                # Ensure boundaries are within the image dimensions\n",
    "\n",
    "                if (minr <0) or (minc <0) or (maxr >im.shape[1]) or (maxc >im.shape[2]):\n",
    "                    pad_top = max(0, -minr)\n",
    "                    minr = max(0, minr)\n",
    "\n",
    "                    pad_bottom = max(0, maxr - im.shape[1])\n",
    "                    maxr = min(maxr, im.shape[1])\n",
    "\n",
    "                    pad_left = max(0, -minc)\n",
    "                    minc = max(0, minc)\n",
    "\n",
    "                    pad_right = max(0, maxc - im.shape[2])\n",
    "                    maxc = min(maxc, im.shape[2])\n",
    "\n",
    "                # Crop and pad the image if needed\n",
    "\n",
    "                    patch = np.pad(im[:, minr:maxr, minc:maxc],\n",
    "                                ((0, 0), (pad_top, pad_bottom), (pad_left, pad_right)),\n",
    "                                mode='constant', constant_values=0)\n",
    "                else:\n",
    "                    # print(minr,maxr,minc,maxc)\n",
    "                    patch = im[:, minr:maxr, minc:maxc]\n",
    "                    print(patch.shape)\n",
    "                # patch = im[:, minr:maxr, minc:maxc]\n",
    "\n",
    "                patch = Image.fromarray(np.transpose(patch,(2,1,0)))\n",
    "                if patch.size !=(32,32):\n",
    "                    patch=patch.resize((32,32))\n",
    "                # except:\n",
    "                #     print( minr,maxr,minc,maxc, im.shape)\n",
    "\n",
    "                # patch = transforms.ToTensor()(patch)\n",
    "                patches_list.append(patch)\n",
    "        print(len(patches_list))\n",
    "        patches_tensor= np.stack(patches_list)\n",
    "        print(f'Patches: {patches_tensor.shape}')\n",
    "        save_dir= f'{crop_path}/{r*2}'\n",
    "        os.makedirs(f'{save_dir}/{group}',exist_ok=True)\n",
    "        np.save(f'{save_dir}/{group}/{name}.npy',patches_tensor)\n",
    "        im=None\n",
    "        patches_tensor=None\n",
    "        sdata=None\n",
    "        del im,patches_tensor\n",
    "        print(f\"Saved {len(patches_list)} patches to the list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uao_jTATGPZy"
   },
   "outputs": [],
   "source": [
    "# np.save(f'{save_dir}/{group}/{name}.npy',patches_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kaFLZq4GPZy"
   },
   "source": [
    "## Process Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3NQ2Gu9GPZy"
   },
   "source": [
    "You will need to download public pretrain model from https://huggingface.co/MahmoodLab/UNI (file: pytorch_model.bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLy1mu8vGPZz"
   },
   "outputs": [],
   "source": [
    "# pretrain_model_path='./resource/pretrain/pytorch_model.bin'\n",
    "# preprocessed_path= './preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-95lIbRhGPZz"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch #1.13.1\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3xl_Pm9GPZz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self, numpy_folder, augmentation=True, random_seed=1234, k=5, split= False,\n",
    "                 list= ['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I']):\n",
    "        self.augmentation = augmentation\n",
    "        # loc_list = sorted([os.path.basename(name) for name in glob.glob(os.path.join(image_folder, '*'))])\n",
    "        # random.seed(random_seed)\n",
    "        # random.shuffle(loc_list)\n",
    "\n",
    "        # partitions = self.partition(loc_list, k)\n",
    "        # selected_loc_list = [i for m in m_list for i in partitions[m]]\n",
    "\n",
    "        # self.image_list = [\n",
    "        #     img for loc in selected_loc_list for img in glob.glob(os.path.join(image_folder, loc, '*.png'))\n",
    "        # ]\n",
    "        NAMES = list\n",
    "        # NAMES= NAMES[:1]\n",
    "\n",
    "        tensors_list=[\n",
    "                np.load(f'{numpy_folder}/{name}.npy') for name in NAMES\n",
    "    ]\n",
    "        # except:\n",
    "        #     tensors_list=[\n",
    "        #          torch.load(f'{numpy_folder}/{name}.pt') for name in NAMES\n",
    "        # ]\n",
    "        tensors_list= np.vstack(tensors_list)\n",
    "        # for num in tensors_list:\n",
    "        #     num= transforms.ToTensor()(num)\n",
    "        # tensors_list=torch.from_numpy(tensors_list)\n",
    "        self.tensors_list = tensors_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tensor =transforms.ToTensor()(self.tensors_list[index])\n",
    "\n",
    "        if self.augmentation:\n",
    "            # Flip upside-down (vertically)\n",
    "            if random.random() < 0.5:\n",
    "                tensor = torch.flip(tensor, dims=[1])  # Flip along the height dimension\n",
    "\n",
    "            # Flip left-to-right (horizontally)\n",
    "            if random.random() < 0.5:\n",
    "                tensor = torch.flip(tensor, dims=[2])\n",
    "\n",
    "        # img = torch.from_numpy(img).permute(2, 0, 1)  # HWC to CHW\n",
    "        return tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def partition(lst, n):\n",
    "        division = len(lst) / float(n)\n",
    "        return [lst[int(round(division * i)): int(round(division * (i + 1)))] for i in range(n)]\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self,resources_path):\n",
    "        super().__init__()\n",
    "        model = timm.create_model(\n",
    "    \"vit_large_patch16_224\", img_size=224, patch_size=16, init_values=1e-5, num_classes=0, dynamic_img_size=True\n",
    ")\n",
    "        pretrain_model_path=f'{resources_path}/pytorch_model.bin'\n",
    "        model.load_state_dict(torch.load((f'{pretrain_model_path}')), strict=True)\n",
    "        # model.load_state_dict(torch.load((\"D:/Downloads/pytorch_model.bin\"), map_location=\"cuda:0\"), strict=True)\n",
    "        self.model=model\n",
    "        # self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x.unsqueeze(0) # for testing single image\n",
    "        # print(x.shape)\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1XTx0j_GPZz"
   },
   "outputs": [],
   "source": [
    "# tensor_folder = f'{crop_path}/{int(r*2)}'\n",
    "# save_dir=f'{preprocessed_path}/{int(r*2)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3YBrAT-GPZz"
   },
   "outputs": [],
   "source": [
    "# NAMES = ['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I']\n",
    "def preprocess_patches(NAMES,tensor_folder,save_dir,resources_path='/resources',evel= False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoder= ImageEncoder(resources_path=resources_path)\n",
    "    encoder.eval()\n",
    "    encoder=encoder.to(device)\n",
    "    emb_stack=[]\n",
    "    # from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "    # local_index = 0\n",
    "    # label_map = defaultdict(list)\n",
    "\n",
    "    for name in NAMES:\n",
    "\n",
    "\n",
    "        if evel==False:\n",
    "            label= 'train'\n",
    "        else:\n",
    "             label= 'evel'\n",
    "\n",
    "        if os.path.exists(f'{save_dir}/{label}/{name}.npy'):\n",
    "            continue\n",
    "\n",
    "        data_dir=tensor_folder+f'/{label}'\n",
    "        dataset = DataGenerator(data_dir,list=[name])\n",
    "        # print(dataset[[0,1,2]],indices[train_mask].shape)\n",
    "\n",
    "        comb_set={}\n",
    "\n",
    "\n",
    "        emb_stack=[]\n",
    "        # if label== 'test' and name== NAMES[5]:\n",
    "        #     continue\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=200, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i=0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                emb= encoder(batch)\n",
    "                emb_stack.append(emb.cpu())\n",
    "                i+=1\n",
    "                if i%10 ==0:\n",
    "                    print(i)\n",
    "\n",
    "        emb_stack = torch.cat(emb_stack, dim=0)\n",
    "        print(label,emb_stack.shape)\n",
    "        emb_stack=emb_stack.numpy()\n",
    "        os.makedirs(f'{save_dir}/{label}',exist_ok=True)\n",
    "        np.save(f'{save_dir}/{label}/{name}.npy',emb_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mh87_yw-GPZ0"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from types import SimpleNamespace\n",
    "from torch_geometric.data import Data\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_edge_index(dataframe, k=6):\n",
    "    # \"\"\"\n",
    "    # Calculate edge indices for a k-neighbor graph based on distances.\n",
    "    \n",
    "    # Args:\n",
    "    #     dataframe (pd.DataFrame): DataFrame with 'x' and 'y' columns.\n",
    "    #     k (int): Number of neighbors (default is 6).\n",
    "    \n",
    "    # Returns:\n",
    "    #     edge_index (list of tuples): List of edges as (source, target).\n",
    "    # \"\"\"\n",
    "    # Extract coordinates\n",
    "        coordinates = dataframe[['x', 'y']].to_numpy()  # Shape: (n, 2)\n",
    "        \n",
    "        # Calculate pairwise distances\n",
    "        dist_matrix = distance.cdist(coordinates, coordinates, metric='euclidean')  # Shape: (n, n)\n",
    "        \n",
    "        # For each point, find indices of k nearest neighbors (excluding itself)\n",
    "        n = len(coordinates)\n",
    "        edge_index = []\n",
    "        for i in range(n):\n",
    "            # Get indices of sorted distances (excluding itself)\n",
    "            neighbors = np.argsort(dist_matrix[i])[1:k+1]\n",
    "            \n",
    "            # Create edges from i to each neighbor\n",
    "            for neighbor in neighbors:\n",
    "                edge_index.append((i, neighbor))  # (source, target)\n",
    "                edge_index.append((neighbor, i))\n",
    "            edge_index.append((-1,i))\n",
    "        # edge_index= np.array(edge_index)\n",
    "        \n",
    "        # edge_index+=1\n",
    "        return edge_index\n",
    "\n",
    "\n",
    "class NeuronData_3(Dataset):\n",
    "    def __init__(self,cluster_path= 'E:/DATA/crunch/tmp', emb_folder=f'D:/DATA/Gene_expression/Crunch/preprocessed'\n",
    "                 , augmentation=True,encoder_mode=False, random_seed=1234, train=True, split= False,\n",
    "                 name_list= ['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I'],evel=False):\n",
    "        self.augmentation = augmentation\n",
    "        emb_dir=emb_folder\n",
    "        # emb_dir=  \n",
    "        self.encoder_mode= encoder_mode\n",
    "        NAMES = name_list\n",
    "        group='evel'\n",
    "        dataset_type= None\n",
    "        if train== False and split == True:\n",
    "                dataset_type= 0 \n",
    "        elif train== True and split == True:\n",
    "                dataset_type= 1\n",
    "        elif train== True and split == False:\n",
    "                dataset_type= -1\n",
    "        if split ==True:\n",
    "            group='train'\n",
    "            try:\n",
    "                NAMES.remove('DC1')\n",
    "            except:\n",
    "                print('DC1 is not in list')\n",
    "        if evel ==True:\n",
    "            group='evel'\n",
    "            dataset_type=-1\n",
    "        # NAMES= NAMES[:1]\n",
    "        self.group= group\n",
    "        print(f'Type: {dataset_type}, group: {group}')\n",
    "        centroids_dict={}\n",
    "        valid_clusters_dict={}\n",
    "        valid_cell_list_cluster_dict={}\n",
    "        emb_cells_dict={}\n",
    "        emb_centroids_dict={}\n",
    "        lenngths=[]\n",
    "        cluster_edge_index_dict={}\n",
    "        cluster_index_cells_list_in_square_dict={}\n",
    "        \n",
    "        cluster_centroid_exps_dict={}\n",
    "        cluster_cell_exps_dict={}\n",
    "        cluster_cell_mask_dict={}\n",
    "        sorted_indices_dict={}\n",
    "        cell_id_list_dict={}\n",
    "        for name in NAMES:\n",
    "            print(name)\n",
    "            cluster_edge_index=[]\n",
    "            cluster_index_cells_list_in_square=[]\n",
    "            cluster_centroid_exps=[]\n",
    "            cluster_cell_exps=[]\n",
    "            cluster_cell_mask=[]\n",
    "            cell_id_list=[]\n",
    "      \n",
    "            if split== True:\n",
    "                cluster_dir=f'{cluster_path}/cluster/{group}/cluster_data_split'\n",
    "            elif train == False:\n",
    "                cluster_dir=f'{cluster_path}/cluster/{group}/cluster_data'\n",
    "                \n",
    "            with open(f'{cluster_dir}/{name}_cells.pkl','rb') as f:\n",
    "                cell_list_cluster = pickle.load(f)\n",
    "          \n",
    "            \n",
    "            emb_cells= torch.from_numpy(np.load(f'{emb_dir}/24/{group}/{name}.npy'))\n",
    "            emb_centroids= torch.from_numpy(np.load(f'{emb_dir}/80/{group}/{name}.npy')) # len of valid_clusters(['group'] != -1) = len of emb_centroids\n",
    "\n",
    "            # centroids = kmeans.cluster_centers_ # n x [x,y]\n",
    "            centroids = cell_list_cluster.groupby('cluster')[['x', 'y']].mean().sort_index().reset_index().to_numpy()\n",
    "            # Filter out invalid clusters (those with 'train' = -1)\n",
    "            distances = cdist(centroids, centroids)\n",
    "\n",
    "            # Initialize variables\n",
    "            \n",
    "            if group =='train':\n",
    "                valid_cell_list_cluster= cell_list_cluster[cell_list_cluster[group] != -1]\n",
    "                # len of emb_cells == len of cell_list_cluster\n",
    "                emb_cells= emb_cells[cell_list_cluster[group] != -1] # len of emb_cells == len of valid_cell_list_cluster\n",
    "            else:\n",
    "                valid_cell_list_cluster= cell_list_cluster\n",
    "                \n",
    "            valid_clusters = valid_cell_list_cluster['cluster'].unique()    \n",
    "            # filtered_cells_index = [i for i in range(len(cell_list_org)) if cell_list_org[i]['label'] == group]\n",
    "            \n",
    "   \n",
    "            if dataset_type != None:\n",
    "                if dataset_type ==1 :\n",
    "                    temp_cluster            =cell_list_cluster[cell_list_cluster[group] == 1]['cluster'].unique()\n",
    "                    cluster_index           =[list(valid_clusters).index(cluster_id) for cluster_id in temp_cluster ]\n",
    "                    emb_centroids           =emb_centroids[cluster_index]\n",
    "                    emb_cells               =emb_cells[valid_cell_list_cluster[group].to_numpy() == 1]\n",
    "                    valid_cell_list_cluster =valid_cell_list_cluster[valid_cell_list_cluster[group] == 1]\n",
    "                    valid_clusters          =temp_cluster\n",
    "                elif dataset_type ==0:\n",
    "                    temp_cluster            =cell_list_cluster[cell_list_cluster[group] == 0]['cluster'].unique()\n",
    "                    cluster_index           =[list(valid_clusters).index(cluster_id) for cluster_id in temp_cluster ]\n",
    "                    emb_centroids           =emb_centroids[cluster_index]\n",
    "                    emb_cells               =emb_cells[valid_cell_list_cluster[group].to_numpy() == 0]\n",
    "                    valid_cell_list_cluster =valid_cell_list_cluster[valid_cell_list_cluster[group] == 0]\n",
    "                    valid_clusters          =temp_cluster\n",
    "          \n",
    "            centroids= centroids[valid_clusters]   \n",
    "            ''''''\n",
    "            n_centroids = len(centroids)\n",
    "            visited = [False] * n_centroids\n",
    "            sorted_indices = []\n",
    "            current = 0  # Start from the first centroid\n",
    "\n",
    "            for _ in range(n_centroids):\n",
    "                # Mark current centroid as visited\n",
    "                visited[current] = True\n",
    "                sorted_indices.append(current)\n",
    "\n",
    "                # Find the next closest unvisited centroid\n",
    "                unvisited_distances = [(i, distances[current, i]) for i in range(n_centroids) if not visited[i]]\n",
    "                if unvisited_distances:\n",
    "                    current = min(unvisited_distances, key=lambda x: x[1])[0]\n",
    "\n",
    "            # Use the sorted indices to get sorted centroids\n",
    "            sorted_indices_dict[name]=sorted_indices\n",
    "            ''''''\n",
    "            lenngths.append(len(centroids))\n",
    "            centroids_dict[name]            =centroids\n",
    "            valid_clusters_dict[name]       =valid_clusters\n",
    "            emb_cells_dict[name]            =emb_cells\n",
    "            emb_centroids_dict[name]        =emb_centroids\n",
    "            #####################################################\n",
    "            \n",
    "            for i in range(len(valid_clusters)):\n",
    "                cell_ids= valid_cell_list_cluster['cell_id'].to_numpy()\n",
    "                cell_id_list.append(cell_ids)\n",
    "                cell_mask=(valid_cell_list_cluster['cluster'].to_numpy() == valid_clusters[i])\n",
    "                cluster_cell_mask.append(cell_mask)\n",
    "                \n",
    "                cells_list_in_cluster = valid_cell_list_cluster[valid_cell_list_cluster['cluster'] == valid_clusters[i]]\n",
    "                edge_index=get_edge_index(cells_list_in_cluster,k=6)\n",
    "                cluster_edge_index.append(edge_index)\n",
    "                x_center, y_center = centroids[i,1],centroids[i,2]\n",
    "                if group == 'train':\n",
    "                    half_side = int(256 / 2)\n",
    "                    x_min, x_max = x_center - half_side, x_center + half_side\n",
    "                    y_min, y_max = y_center - half_side, y_center + half_side\n",
    "                    \n",
    "                    index_cells_list_in_square =(\n",
    "                    (cells_list_in_cluster['x'] >= x_min) & (cells_list_in_cluster['x'] <= x_max) &\n",
    "                    (cells_list_in_cluster['y'] >= y_min) & (cells_list_in_cluster['y'] <= y_max)\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "                    ############\n",
    "                    cells_list_in_square=cells_list_in_cluster[index_cells_list_in_square]\n",
    "                    if len(cells_list_in_square)==0:\n",
    "                        cells_list_in_square=cells_list_in_cluster\n",
    "                    centroid_exps=  np.array([np.sum(cells_list_in_square['counts'].to_numpy()/len(cells_list_in_square), axis=0)])\n",
    "                    normalized_counts = centroid_exps / centroid_exps.sum(axis=1, keepdims=True) * 100\n",
    "                    centroid_exps = np.log1p(normalized_counts)\n",
    "                    cells_list_in_square=None\n",
    "                    del cells_list_in_square\n",
    "                    \n",
    "                    cell_exps= np.stack(cells_list_in_cluster['counts'].to_numpy())\n",
    "                    normalized_counts = cell_exps / cell_exps.sum(axis=1, keepdims=True) * 100\n",
    "                    cell_exps = np.log1p(normalized_counts)\n",
    "                    \n",
    "                    cells_list_in_cluster=None\n",
    "                    del cells_list_in_cluster\n",
    "                else:\n",
    "                    centroid_exps=np.empty(460)\n",
    "                    cell_exps=np.empty(460)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                cluster_centroid_exps.append(centroid_exps) \n",
    "                centroid_exps=None\n",
    "                cluster_cell_exps.append(cell_exps)\n",
    "                cell_exps=None\n",
    "                del centroid_exps,cell_exps\n",
    "            \n",
    "            cell_id_list_dict[name]=cell_id_list\n",
    "            cluster_cell_mask_dict[name]=cluster_cell_mask    \n",
    "            cluster_centroid_exps_dict[name]= cluster_centroid_exps\n",
    "            cluster_cell_exps_dict[name]= cluster_cell_exps\n",
    "            \n",
    "            cluster_centroid_exps=None\n",
    "            cluster_cell_exps=None\n",
    "            valid_cell_list_cluster=None\n",
    "            cluster_cell_mask=None\n",
    "            del valid_cell_list_cluster,cluster_cell_mask\n",
    "                \n",
    "                ##############\n",
    "                \n",
    "            cluster_index_cells_list_in_square_dict[name]=cluster_index_cells_list_in_square\n",
    "            cluster_edge_index_dict[name]= cluster_edge_index\n",
    "            cluster_edge_index=None\n",
    "            cluster_index_cells_list_in_square=None\n",
    "               \n",
    "        self.cell_id_list_dict      = cell_id_list_dict \n",
    "        self.cluster_cell_mask_dict= cluster_cell_mask_dict\n",
    "        self.cluster_cell_exps_dict = cluster_cell_exps_dict\n",
    "        self.cluster_centroid_exps_dict= cluster_centroid_exps_dict\n",
    "        self.cluster_edge_index_dict= cluster_edge_index_dict\n",
    "        self.centroids_dict             =centroids_dict\n",
    "        self.valid_clusters_dict        =valid_clusters_dict\n",
    "        self.emb_cells_dict             =emb_cells_dict\n",
    "        self.emb_centroids_dict         =emb_centroids_dict\n",
    "        self.lengths                    =lenngths\n",
    "        self.sorted_indices_dict        =sorted_indices_dict\n",
    "        self.cumlen =np.cumsum(self.lengths)\n",
    "        self.id2name = dict(enumerate(NAMES))\n",
    "        \n",
    "        cluster_cell_mask_dict=None\n",
    "        cluster_cell_exps_dict=None\n",
    "        cluster_centroid_exps_dict=None\n",
    "        valid_cell_list_cluster=None\n",
    "        cell_list_cluster   =None\n",
    "        emb_cells           =None\n",
    "        emb_centroids       =None\n",
    "        self.global_to_local = self._create_global_index_map()\n",
    "    def __len__(self):\n",
    "        return self.cumlen[-1]\n",
    "    \n",
    "    def _create_global_index_map(self):\n",
    "        \"\"\"Create a mapping from global index to (dataset index, local index).\"\"\"\n",
    "        global_to_local = []\n",
    "        for i, name in enumerate(self.id2name):\n",
    "            local_indices = list(zip([i] * self.lengths[i], range(self.lengths[i])))\n",
    "            global_to_local.extend(local_indices)\n",
    "        return global_to_local\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "     \n",
    "        i, idx = self.global_to_local[index]\n",
    "        idx= self.sorted_indices_dict[self.id2name[i]][idx]\n",
    "        centroid= self.centroids_dict[self.id2name[i]][idx]\n",
    "        cluster_id= self.valid_clusters_dict[self.id2name[i]][idx]\n",
    "        cell_ids= self.cell_id_list_dict[self.id2name[i]][idx]\n",
    "        \n",
    "        emb_centroid=self.emb_centroids_dict[self.id2name[i]][idx]\n",
    "        cluster_edge_index= self.cluster_edge_index_dict[self.id2name[i]][idx]\n",
    "        \n",
    "        if self.group =='train':\n",
    "            cell_exps=self.cluster_cell_exps_dict[self.id2name[i]][idx]\n",
    "        else:\n",
    "            cell_exps=np.empty(460)\n",
    "        centroid_exps= self.cluster_centroid_exps_dict[self.id2name[i]][idx]\n",
    "        cluster_cell_mask= self.cluster_cell_mask_dict[self.id2name[i]][idx]\n",
    "        emb_cells_in_cluster= self.emb_cells_dict[self.id2name[i]][cluster_cell_mask]\n",
    "      \n",
    "    \n",
    "        \n",
    "        cell_edge_index= cluster_edge_index\n",
    "      \n",
    "        # return item\n",
    "        if self.encoder_mode==True:\n",
    "            emb_cells_in_cluster=torch.empty(0)\n",
    "            cell_exps=np.array([])\n",
    "            cell_edge_index=[]\n",
    "        return  Data(\n",
    "            x=emb_cells_in_cluster,  # Node features (including centroid and cells)\n",
    "            edge_index=cell_edge_index,  # Edge indices (intra-cluster and centroid-to-cell)\n",
    "            emb_centroid=emb_centroid,  # Embedding for centroid\n",
    "            cluster_centroid=centroid, # Centroid coordinates\n",
    "            cell_exps=cell_exps,\n",
    "            centroid_exps=centroid_exps,\n",
    "            cell_num= len(emb_cells_in_cluster),\n",
    "            cell_ids= cell_ids\n",
    "        )\n",
    "        # return emb_centroid, GE_centroid, centroid, emb_cells_in_cluster, cell_exps, cell_edge_index\n",
    "\n",
    "       \n",
    "\n",
    "    # @staticmethod\n",
    "    # def partition(lst, n):\n",
    "    #     division = len(lst) / float(n)\n",
    "    #     return [lst[int(round(division * i)): int(round(division * (i + 1)))] for i in range(n)]\n",
    " \n",
    "def build_batch_graph(batch,device,centroid_layer):\n",
    "    \"\"\"Build a graph for a batch of 400 clusters.\"\"\"\n",
    "    all_x = []  # Store node features\n",
    "    all_edge_index = []  # Store edges\n",
    "    cluster_centroids = []  # Store centroids for all clusters\n",
    "    centroid_exps=[]\n",
    "    cell_exps=[]\n",
    "    all_centroids_emb=[]\n",
    "    node_offset = 0  # To adjust indices for each cluster\n",
    "    # print(batch.x.shape)\n",
    "    centroid_exps = np.vstack(batch.centroid_exps)  # Get centroid expression data\n",
    "    # print(centroid_exps.shape)\n",
    "    # print(batch.x.shape, batch.emb_centroid)\n",
    "    if batch.x.shape[0]!=0:\n",
    "        cell_exps = np.vstack(batch.cell_exps)  # Get cell expression data\n",
    "        # print(centroid_exps.shape, cell_exps.shape)\n",
    "        exps= np.vstack((centroid_exps, cell_exps))\n",
    "        for i in range(len(batch.edge_index)):\n",
    "            # print(torch.tensor(batch.edge_index[i]).shape)\n",
    "            all_edge_index.append(torch.tensor(batch.edge_index[i]) + node_offset + len(batch.edge_index))\n",
    "            # print(torch.tensor(edge_index).shape,len(edge_index))\n",
    "            node_offset += batch.cell_num[i]\n",
    "        edge_index = torch.cat(all_edge_index, dim=0)\n",
    "        # emb_centroids=batch.emb_centroid\n",
    "        batch.emb_centroid= batch.emb_centroid.view(-1,1024)\n",
    "        all_x = torch.cat([batch.emb_centroid, batch.x], dim=0)\n",
    "    else:\n",
    "        exps=centroid_exps\n",
    "        all_edge_index = []\n",
    "        # print(len(edge_index))\n",
    "        # print(edge_index.size(0),batch.edge_index.size(0))\n",
    "        edge_index=torch.empty(0, dtype=torch.long)\n",
    "        all_x = batch.emb_centroid\n",
    "    # print(all_x.shape,centroid_exps.shape)\n",
    "    # print(edge_index.shape)\n",
    "    # all_x = batch.x  # All node features (batch size x feature_dim)\n",
    "    \n",
    "\n",
    "    # Assuming centroid embeddings are stored as a separate tensor\n",
    "    # emb_centroids = batch.emb_centroid  # Stack centroid embeddings from the batch\n",
    "    cluster_centroids = torch.tensor(np.array(batch.cluster_centroid))  # Get centroid positions\n",
    "    \n",
    "   \n",
    "    dist_matrix = torch.cdist(cluster_centroids, cluster_centroids)\n",
    "    \n",
    "  \n",
    "    k=min(6,len(cluster_centroids))\n",
    "    neighbors = torch.topk(dist_matrix, k=k, largest=False).indices  # Top 6 neighbors\n",
    "\n",
    "    # Add inter-cluster edges\n",
    "    edge_index_centroid = []\n",
    "    for i in range(len(cluster_centroids)):\n",
    "        add= torch.stack([torch.full((k,), i), neighbors[i]])\n",
    "        edge_index_centroid.append(add)\n",
    "    edge_index_centroid = torch.cat(edge_index_centroid, dim=1)\n",
    "    edge_index_centroid= edge_index_centroid.T\n",
    "    edge_index = torch.cat([edge_index, edge_index_centroid], dim=0)\n",
    "    if centroid_layer ==False:\n",
    "        edge_index_centroid =None\n",
    "   \n",
    "    return Data(x=all_x.to(device), edge_index=edge_index.to(device), edge_index_centroid=edge_index_centroid),exps,centroid_exps\n",
    "\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "class LR_Scheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer:Optimizer,\n",
    "                 num_epochs:int, iter_per_epoch:int, warmup_epochs:int,\n",
    "                 warmup_lr:float, base_lr:float, final_lr:float,\n",
    "                 constant_predictor_lr=False):\n",
    "\n",
    "        assert num_epochs > 0 and iter_per_epoch > 0 and warmup_epochs >= 0\n",
    "        assert warmup_epochs <= num_epochs\n",
    "        assert warmup_lr >= 0 and base_lr >= 0 and final_lr >= 0\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.constant_predictor_lr = constant_predictor_lr\n",
    "\n",
    "        warmup_iter = iter_per_epoch * warmup_epochs\n",
    "        warmup_lr_schedule = np.linspace(warmup_lr, base_lr, warmup_iter)\n",
    "        decay_iter = iter_per_epoch * (num_epochs - warmup_epochs)\n",
    "        cosine_lr_schedule = final_lr + 0.5*(base_lr - final_lr) * (1 + np.cos(np.pi*np.arange(decay_iter) / decay_iter))\n",
    "\n",
    "        self.lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
    "        self.optimizer = optimizer\n",
    "        self.iter = -1\n",
    "        self.current_lr = 0\n",
    "\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def step(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "\n",
    "            if self.constant_predictor_lr and param_group['name'] == 'predictor':\n",
    "                param_group['lr'] = self.base_lr\n",
    "            else:\n",
    "                lr = param_group['lr'] = self.lr_schedule[self.iter]\n",
    "\n",
    "        self.iter += 1\n",
    "        self.current_lr = lr\n",
    "        return lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXOOfc0vGPZ0"
   },
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INEZMxmhGPZ0"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer,scheduler, args, filename=\"checkpoint.pth.tar\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'args': args\n",
    "    }\n",
    "    dir=f\"{args.save_dir}\"\n",
    "    if args.encoder_mode== True:\n",
    "        dir=f\"{args.save_dir}/encoder\"\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    torch.save(checkpoint, f\"{dir}/{filename}\")\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "import sys\n",
    "\n",
    "def load_checkpoint(epoch, model, optimizer,scheduler,args):\n",
    "    filename=f\"checkpoint_epoch_{epoch}.pth.tar\"\n",
    "    dir=f\"{args.save_dir}\"\n",
    "    if args.encoder_mode== True:\n",
    "        dir=f\"{args.save_dir}/encoder\"\n",
    "    checkpoint = torch.load(f\"{dir}/{filename}\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # epoch = checkpoint['epoch']\n",
    "    # args = checkpoint['args']\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(f\"Checkpoint loaded from epoch {epoch}\")\n",
    "    return epoch + 1, args,model,scheduler,optimizer\n",
    "def train_one_epoch(model, train_loader, optimizer,scheduler, device, epoch,demo=False,centroid_layer=False):\n",
    "    model.train()\n",
    "    total_loss = torch.zeros(1).to(device)\n",
    "    train_loader = tqdm(train_loader, file=sys.stdout, ncols=100, colour='red')\n",
    "    # optimizer.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    accumulation_steps = 1  # Number of steps to accumulate gradients\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # graph_data= build_batch_graph(data,device,centroid_layer=centroid_layer)\n",
    "        # data=None\n",
    "        # pred,label,pred_c,label_c = model(graph_data)\n",
    "        if i!=-1: #for testing manual\n",
    "            \n",
    "            # data = data.to(device)\n",
    "            \n",
    "            graph_data= build_batch_graph(data,device,centroid_layer=centroid_layer)\n",
    "            data=None\n",
    "            # print(graph_data.exps.shape)\n",
    "            # excep?t:\n",
    "                # print(data)\n",
    "            # graph_data.cpu()\n",
    "            # data.cpu()\n",
    "            pred,label,pred_c,label_c = model(graph_data)\n",
    "            \n",
    "            label = np.array(label, dtype=np.float32)\n",
    "            label = torch.from_numpy(label)\n",
    "            label= label.to(device)\n",
    "            # print(label.shape,pred.shape)\n",
    "\n",
    "            # print(type(label))\n",
    "            if pred_c is not None:\n",
    "                label_c = np.array(label_c, dtype=np.float32)\n",
    "                label_c= torch.from_numpy(label_c)\n",
    "                label_c= label_c.to(device)\n",
    "                loss = F.mse_loss(pred, label)+ F.mse_loss(pred_c, label_c)\n",
    "            else:\n",
    "                loss = F.mse_loss(pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                optimizer.step()  # Perform optimizer step\n",
    "                scheduler.step()  # Adjust learning rate\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # optimizer.step()\n",
    "            # scheduler.step()\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "            total_loss = (total_loss * i + loss.detach()) / (i + 1)\n",
    "            if i%3==0:\n",
    "                train_loader.desc = 'Train\\t[epoch {}] lr: {}\\tloss {}'.format(epoch, optimizer.param_groups[0][\"lr\"], round(total_loss.item(), 3))\n",
    "            # if i==3 and demo==True:\n",
    "        #     break\n",
    "    torch.cuda.empty_cache()\n",
    "    return pred\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_one_epoch(model, val_loader, device, centroid,demo=False, encoder_mode =False, data_type='val',centroid_layer=False):\n",
    "    model.eval()\n",
    "    labels = torch.tensor([], device=device)\n",
    "    preds = torch.tensor([], device=device)\n",
    "    if data_type == 'val':\n",
    "        val_loader = tqdm(val_loader, file=sys.stdout, ncols=100, colour='blue')\n",
    "    elif data_type == 'test':\n",
    "        val_loader = tqdm(val_loader, file=sys.stdout, ncols=100, colour='green')\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        # data = data.to(device)\n",
    "        graph_data= build_batch_graph(data,device,centroid_layer)\n",
    "        # data.cpu()\n",
    "        # graph_data.cpu()\n",
    "        if encoder_mode ==True:\n",
    "            centroid=0\n",
    "        output,label,_,_= model(graph_data)\n",
    "        output= output[centroid:]\n",
    "        label= label[centroid:]\n",
    "        \n",
    "        label = torch.from_numpy(label).to(device)\n",
    "        labels = torch.cat([labels.cpu(), label.cpu()], dim=0)\n",
    "        preds = torch.cat([preds.cpu(), output.detach().cpu()], dim=0)\n",
    "        # if i==3 and demo==True:\n",
    "        #     break\n",
    "    print(labels.shape,preds.shape)\n",
    "    return preds.cpu(), labels.cpu()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4k6xI5xGPZ1"
   },
   "outputs": [],
   "source": [
    "class GATModel_3(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=1024, output_dim=1024, num_heads=3,n_classes=460,centroid_layer=False):\n",
    "        super(GATModel_3, self).__init__()\n",
    "\n",
    "        # MLP for flattening emb_cells_in_cluster\n",
    "       \n",
    "        # GATConv for graph processing\n",
    "        self.centroid_layer=centroid_layer\n",
    "        self.gat_conv_centroid = GATv2Conv(input_dim, hidden_dim, heads=num_heads, concat=False)\n",
    "        self.gat_conv = GATv2Conv(input_dim, hidden_dim, heads=num_heads, concat=False)\n",
    "        self.activate = F.elu\n",
    "        self.fc = nn.Linear(hidden_dim, n_classes)\n",
    "    def forward(self, data):\n",
    "        emb_data, exps, exps_c= data\n",
    "        edge_index = emb_data.edge_index \n",
    "        h_c=None\n",
    "        \n",
    "        x=emb_data.x\n",
    "      \n",
    "\n",
    "        # Apply GATConv across the entire batch graph\n",
    "        \n",
    "        \n",
    "        h = self.gat_conv(x, edge_index.T)\n",
    "        h = self.fc(h).squeeze(0)\n",
    "        \n",
    "        # print(h.shape,exps.shape)\n",
    "        return h,exps,h_c,exps_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9PMogRdd22n"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:12:44.782263Z",
     "start_time": "2024-10-28T20:12:44.773022Z"
    },
    "id": "tRb9yLCcZW5y"
   },
   "outputs": [],
   "source": [
    "# In the training function, users build and train the model to make inferences on the test data.\n",
    "# Your model must be stored in the resources/ directory.\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def train(\n",
    "    data_directory_path: str\n",
    "    ,model_directory_path:str\n",
    "):\n",
    "    # no train\n",
    "    no_train= False\n",
    "    if no_train == True:\n",
    "        return\n",
    "    NAMES = ['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I']\n",
    "    sdata_dir=data_directory_path\n",
    "    # sdata_dir=f'F:/DATA/crunch_large/zip_server'\n",
    "    pre_load_path= '/tmp/pre_load'\n",
    "    os.makedirs(pre_load_path,exist_ok=True)\n",
    "    create_pre_load(pre_load_path=pre_load_path,sdata_dir= sdata_dir,name_list=NAMES)\n",
    "    cluster_path= '/tmp/cluster'\n",
    "    create_cluster('train',n_centroid=4,NAMES=NAMES,pre_load_path=pre_load_path,cluster_path=cluster_path)\n",
    "    split_cluster(cluster_path=cluster_path,NAMES=NAMES)\n",
    "\n",
    "    crop_path='/tmp/patches'\n",
    "    r=24/2\n",
    "    crop_patches(sdata_dir=sdata_dir,pre_load_path=pre_load_path,crop_path=crop_path,r=r,name_list=NAMES)\n",
    "    pretrain_model_path='/resources/pytorch_model.bin'\n",
    "    preprocessed_path= '/tmp/preprocessed'\n",
    "    tensor_folder = f'{crop_path}/{int(r*2)}'\n",
    "    save_dir=f'{preprocessed_path}/{int(r*2)}'\n",
    "    preprocess_patches(NAMES=NAMES,tensor_folder=tensor_folder\n",
    "                              ,save_dir=save_dir\n",
    "                                 ,resources_path=model_directory_path\n",
    "                                  ,evel=False)\n",
    "\n",
    "    args_dict = {\n",
    "    'batch_size': 73,\n",
    "    'epochs': 10,  # 90\n",
    "    'seed': 2023,  # temperature\n",
    "    'n_workers': 0,  # projection_dim\n",
    "    'n_classes': 460,  # attention heads num\n",
    "    'start_epoch': 0,  # attention heads dim\n",
    "    'heads_layers': 2,  # attention heads layer num\n",
    "    'dropout': 0.0,  # dropout\n",
    "    'encoder_mode': False,  # dataset\n",
    "    'demo': False,  # model saved path\n",
    "    'resume': False,  # resume training\n",
    "    'local': True,  # patch_size\n",
    "    'centroid_layer': 'True',  # patch_size(n)-epoch(e)\n",
    "    'embed_dir': f'{save_dir}',\n",
    "    'demo': False,\n",
    "    'device': 'cuda:0',\n",
    "    'local': True,\n",
    "    'save_dir': model_directory_path\n",
    "}\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "    train_NAMES= NAMES\n",
    "    if args.demo== True:\n",
    "        NAMES=NAMES[:2]\n",
    "        train_NAMES=NAMES[:2]\n",
    "    dir=args.embed_dir\n",
    "    # dir='D:/DATA/Gene_expression/Crunch/preprocessed'\n",
    "    if args.local == True:\n",
    "        pin= False\n",
    "    else:\n",
    "        pin= True\n",
    "    traindata= NeuronData_3(emb_folder=dir,train=True, split =True\n",
    "                          ,name_list= train_NAMES\n",
    "                          ,encoder_mode=args.encoder_mode\n",
    "                          )\n",
    "    train_dataLoader =DataLoader(traindata, batch_size=args.batch_size, shuffle=False,pin_memory=pin)    \n",
    "    # print(len(train_dataLoader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # traindata[2127]\n",
    "    model=GATModel_3(centroid_layer=args.centroid_layer)\n",
    "    model= model.to('cuda')\n",
    "    #------------------------\n",
    "    \n",
    "\n",
    "    # print(f'Using fold {args.fold}')\n",
    "    # print(f'valid: {len(val_set)}')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-5)\n",
    "    scheduler = LR_Scheduler(optimizer=optimizer\n",
    "                             ,num_epochs=args.epochs\n",
    "                             ,base_lr=0.00018\n",
    "                             ,iter_per_epoch = len(train_dataLoader)\n",
    "                             ,warmup_epochs= 10\n",
    "                            ,warmup_lr= 0.00015\n",
    "                            ,final_lr= 0.00005\n",
    "                            ,constant_predictor_lr=False\n",
    ")\n",
    "    val_set= [NeuronData_3(emb_folder=dir,train=False, split =True,name_list= [name],encoder_mode=args.encoder_mode) \n",
    "              for name in NAMES]\n",
    "    val_loader =[DataLoader(set, batch_size=args.batch_size, shuffle=False,pin_memory=pin)for set in val_set]    \n",
    "    output_dir = args.save_dir\n",
    "    \n",
    "    # val_set = DATA_BRAIN(train=False,r=int(args.patch_size/2), device=args.device)\n",
    "    # val_loader = DataLoader(val_set, batch_size=1024, num_workers=args.n_workers, shuffle=False)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "  \n",
    "    min_val_mse = 200.0\n",
    "    min_val_mae = 200.0\n",
    "    start_epoch= args.start_epoch\n",
    "    if start_epoch >0:\n",
    "        start_epoch, args, model,scheduler, optimizer = load_checkpoint(start_epoch, model, optimizer,scheduler,args)\n",
    "    # for step in range(start_epoch*len(train_dataLoader)):\n",
    "    #     scheduler.step()\n",
    "    print(start_epoch)\n",
    "    print(f'start epoch: {start_epoch}, batch size: {args.batch_size}')\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        checkpoint_filename = f\"checkpoint_best_epoch_{epoch}.pth.tar\"\n",
    "        train_logits = train_one_epoch(model=model, train_loader=train_dataLoader\n",
    "                                       ,demo=args.demo, optimizer=optimizer,scheduler=scheduler, device=device, epoch=epoch + 1,centroid_layer=args.centroid_layer)\n",
    "        if (epoch+1)%8 ==0: \n",
    "           \n",
    "            mse_list = []\n",
    "            mae_list = []\n",
    "            for index in range(len(val_loader)): \n",
    "                val_preds, val_labels = val_one_epoch(model=model,demo=args.demo\n",
    "                                                      , val_loader=val_loader[index]\n",
    "                                                      , device=device, data_type='val'\n",
    "                                                      , centroid= args.batch_size\n",
    "                                                      ,encoder_mode=args.encoder_mode\n",
    "                                                      ,centroid_layer=args.centroid_layer)\n",
    "                mse=mean_squared_error(val_labels, val_preds)\n",
    "                mae=mean_absolute_error(val_labels, val_preds)\n",
    "                ###############\n",
    "                tmp_list=[str(i) for i in range(460)]\n",
    "                val_labels=val_labels.numpy()\n",
    "                val_preds= val_preds.numpy() \n",
    "                \n",
    "                mse_list.append(mse)\n",
    "                mae_list.append(mae)\n",
    "            \n",
    "          \n",
    "        if (epoch+1)%4==0:\n",
    "            checkpoint_filename = f\"checkpoint_epoch_{epoch}.pth.tar\"\n",
    "            save_checkpoint(epoch, model, optimizer,scheduler, args, filename=checkpoint_filename)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAxoyZnEGPZ9"
   },
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_test(epoch, model,args):\n",
    "    try:\n",
    "        filename=f\"checkpoint_epoch_{epoch}.pth.tar\"\n",
    "        dir=f\"{args.save_dir}\"\n",
    "        checkpoint = torch.load(f\"{dir}/{filename}\")\n",
    "    except:\n",
    "        filename=f\"checkpoint_epoch_best_{epoch}.pth.tar\"\n",
    "        dir=f\"{args.save_dir}\"\n",
    "        checkpoint = torch.load(f\"{dir}/{filename}\")\n",
    "    if args.encoder_mode== True:\n",
    "        dir=f\"{args.save_dir}\"\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # epoch = checkpoint['epoch']\n",
    "    # args = checkpoint['args']\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(f\"Checkpoint loaded from epoch {epoch}\")\n",
    "def evel_slide(model, val_loader, device, centroid,demo=False, encoder_mode =False, data_type='val',centroid_layer=False):\n",
    "    model.eval()\n",
    "    preds = torch.tensor([], device=device)\n",
    "    if data_type == 'val':\n",
    "        val_loader = tqdm(val_loader, file=sys.stdout, ncols=100, colour='blue')\n",
    "    elif data_type == 'test':\n",
    "        val_loader = tqdm(val_loader, file=sys.stdout, ncols=100, colour='green')\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        # data = data.to(device)\n",
    "        graph_data= build_batch_graph(data,device,centroid_layer)\n",
    "        # data.cpu()\n",
    "        # graph_data.cpu()\n",
    "        if encoder_mode ==True:\n",
    "            centroid=0\n",
    "        output,_,_,_= model(graph_data)\n",
    "        output= output[centroid:]\n",
    "        \n",
    "        \n",
    "        preds = torch.cat([preds.cpu(), output.detach().cpu()], dim=0)\n",
    "        # if i==3 and demo==True:\n",
    "        #     break\n",
    "    print(preds.shape)\n",
    "    return preds.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOIKYDzsGPZ-"
   },
   "outputs": [],
   "source": [
    "# In the inference function, the trained model is loaded and used to make inferences on a\n",
    "# sample of data that matches the characteristics of the training test.\n",
    "import gc\n",
    "def infer(\n",
    "    data_file_path: str,\n",
    "    model_directory_path: str\n",
    "):\n",
    "    name=os.path.splitext(os.path.basename(data_file_path))[0]\n",
    "    sdata_dir=os.path.dirname(data_file_path)\n",
    "    print(data_file_path, name,sdata_dir)\n",
    "\n",
    "\n",
    "    preprocessed_path= '/tmp/preprocessed'\n",
    "    pre_load_path= '/tmp/pre_load'\n",
    "    os.makedirs(pre_load_path,exist_ok=True)\n",
    "    NAMES= ['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I']\n",
    "    create_pre_load(pre_load_path=pre_load_path,name_list=NAMES,sdata_dir= sdata_dir)\n",
    "\n",
    "    cluster_path= '/tmp/cluster'\n",
    "    create_cluster('evel',n_centroid=600,NAMES=[name],pre_load_path=pre_load_path,cluster_path=cluster_path)\n",
    "    crop_path='/tmp/patches'\n",
    "    r=24/2\n",
    "    crop_patches(sdata_dir=sdata_dir,pre_load_path=pre_load_path,crop_path=crop_path,r=r,evel=True,name_list=[name])\n",
    "    pretrain_model_path='/resources/pytorch_model.bin'\n",
    "    tensor_folder = f'{crop_path}/{int(r*2)}'\n",
    "    save_dir=f'{preprocessed_path}/{int(r*2)}'\n",
    "    preprocess_patches(NAMES=[name],tensor_folder=tensor_folder\n",
    "                       ,save_dir=save_dir\n",
    "                       ,resources_path=model_directory_path\n",
    "                       ,evel=True)\n",
    "\n",
    "    args_dict = {\n",
    "    'batch_size': 73,\n",
    "    'epochs': 10,  # 90\n",
    "    'seed': 2023,  # temperature\n",
    "    'n_workers': 0,  # projection_dim\n",
    "    'n_classes': 460,  # attention heads num\n",
    "    'start_epoch': 0,  # attention heads dim\n",
    "    'heads_layers': 2,  # attention heads layer num\n",
    "    'dropout': 0.0,  # dropout\n",
    "    'encoder_mode': False,  # dataset\n",
    "    'demo': False,  # model saved path\n",
    "    'resume': False,  # resume training\n",
    "    'local': True,  # patch_size\n",
    "    'centroid_layer': 'True',  # patch_size(n)-epoch(e)\n",
    "    'embed_dir': f'{save_dir}',\n",
    "    'demo': False,\n",
    "    'device': 'cuda:0',\n",
    "    'local': True,\n",
    "    'save_dir': model_directory_path\n",
    "}\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # NAMES= ['DC1','DC5', 'UC1_I', 'UC1_NI', 'UC6_I', 'UC6_NI', 'UC7_I', 'UC9_I']\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    dir=args.embed_dir\n",
    " \n",
    "    model=GATModel_3()\n",
    "    model= model.to(device)\n",
    "    val_set= NeuronData_3(emb_folder=dir,cluster_path=cluster_path,train=False, split =True,name_list= [name],encoder_mode=args.encoder_mode,evel=True) \n",
    "             \n",
    "    val_loader =DataLoader(set, batch_size=args.batch_size, shuffle=False,pin_memory=True)\n",
    "    start_epoch= args.start_epoch\n",
    "    start_epoch, args, optimizer = load_checkpoint(start_epoch, model, optimizer,args)\n",
    "    preds = val_one_epoch(model=model,demo=args.demo\n",
    "                                                , val_loader=val_loader\n",
    "                                                , device=device, data_type='val'\n",
    "                                                , centroid= args.batch_size\n",
    "                                                ,encoder_mode=args.encoder_mode)\n",
    "    sdata = sd.read_zarr(data_file_path)\n",
    "    # gene_names = sdata[\"anucleus\"].var.index\n",
    "    try:\n",
    "      gene_names = sdata[\"anucleus\"].var.index\n",
    "    except:\n",
    "      gene_names = sd.read_zarr('/data/DC5.zarr')[\"anucleus\"].var.index\n",
    "\n",
    "    sdata=None\n",
    "    \n",
    "    cell_ids = []\n",
    "    for idx in range(len(val_set)):\n",
    "        data = val_set[idx]  # Access the data at index `idx`\n",
    "        cell_ids.extend(data.cell_ids.tolist())\n",
    "    pred = np.round(pred, 2)\n",
    "\n",
    "    prediction = pd.DataFrame(\n",
    "        itertools.product(cell_ids, gene_names),\n",
    "        columns=[\"cell_id\", \"gene\"]\n",
    "    )\n",
    "\n",
    "    prediction[\"prediction\"] = pred.ravel(order=\"C\")\n",
    "    del pred, sdata\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:31:13.603305Z",
     "start_time": "2024-10-28T20:13:17.622508Z"
    },
    "id": "byoKTO_4ZW5z"
   },
   "outputs": [],
   "source": [
    "# This command is running a local test with your submission\n",
    "# # making sure that your submission can be accepted by the system\n",
    "crunch.test(\n",
    "    no_determinism_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHuqmLtWZW50"
   },
   "source": [
    "Now remember to download this notebook and then submit it at https://hub.crunchdao.com/competitions/broad-1/submit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVGaknEAZW50"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
